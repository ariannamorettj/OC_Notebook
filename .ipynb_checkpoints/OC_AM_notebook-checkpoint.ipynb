{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCitations Notebook\n",
    "### Arianna Moretti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "#### 2022\n",
    "\n",
    "1. [22/02 - 02/03 (Log Data Study)](#entry_1)\n",
    "2. [02/03 - 09/03 (OC Index Software Code Refactoring - NOCI + Mapping Merge)](#entry_2)\n",
    "3. [09/03 - 15/03 (_____________)](#entry_3)\n",
    "4. [15/03 - 22/03 (Prometheus file format study and data extraction)](#entry_4)\n",
    "\n",
    "#### 202*\n",
    "\n",
    "5. [??/?? - ??/?? (????)](#entry_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22/02 - 02/03 (Log Data Study) <a class=\"anchor\" id=\"entry_1\"></a>\n",
    "\n",
    "### Studio Files di Log Raw\n",
    "\n",
    "<ul>\n",
    "    <li>Download cartella Dropbox con file di log raw per 2021</li>\n",
    "    <li>Studio dei dati statistici: <a href=\"https://github.com/opencitations/statistics/tree/master/script\">opencitations statistics repo </a> </li>\n",
    "    <li>Studio del file format Prometheus: <a href=\"https://sysdig.com/blog/prometheus-metrics/\"> https://sysdig.com/blog/prometheus-metrics/</a>, <a href=\"https://prometheus.io/docs/instrumenting/clientlibs/\"> https://prometheus.io/docs/instrumenting/clientlibs/</a>\n",
    "    <li>Esempio di <a href=\"http://opencitations.net/statistics/2022-01\">call API per i dati di Log di Gennaio 2021</a> </li>\n",
    "</ul>\n",
    "\n",
    "#### Prometheus File Format\n",
    "Prometheus is an open source time series database including a collection of client libraries which allow metrics to be published, so to be collected by the metrics server. The Prometheus metrics format is largely adopted and became also an independent project: OpenMetrics, aimed at making this metric format specification a standard. Sysdig Monitor dynamically detects and scrape Prometheus metrics.\n",
    "##### Custom Metrics\n",
    "Source: <a href=\"https://sysdig.com/blog/how-to-instrument-code-custom-metrics-vs-apm-vs-opentracing/\">href=\"https://sysdig.com/blog/how-to-instrument-code-custom-metrics-vs-apm-vs-opentracing/</a>\n",
    "Custom Metrics (JMX, Golang expvar, Prometheus, statsd...) is an approach on how to instrument code to easily monitor the performance and troubleshooting of an application. Typical aspects to be monitored are: the most visited components a web page, the slowest components to load, the difference of speed in loading the frontend and the backend, which factors affect the speed of the process (location, browser, device). The options to monitor those aspects are: using an APM instrument, using OpenTracing libraries, or generating metrics ad-hoc for specific components.\n",
    "\n",
    "##### Comparison between CM, APM and Opentracing\n",
    "\n",
    "|Issue | Custom Metrics | APM | Opentracing |\n",
    "| --- | --- | --- | --- |\n",
    "| Code-related problems | Devs need to provide metrics with performance in code but are not as easy to identify | Yes | Yes |\n",
    "| Infrastructure-related problems | Yes | No | No |\n",
    "| Node and service level aggregation | Yes | No | No |\n",
    "| Standard implementaton | Some languages include a standard way to implement them: (Prometheus, Java JMX, Go expvar, …) | No | Yes |\n",
    "| Allows capacity planning | Yes | No | No |\n",
    "| Allows complete statistical measurements | Yes | No | No |\n",
    "| Cloud Native Computing Foundation standard | Prometheus metrics only | No | Yes |\n",
    "| Distributed application analysis | Yes, without per trace analysis | Yes | Yes |\n",
    "| Useful for developers for pre-production environments | Yes | Yes | Yes |\n",
    "| Useful for complete DevOps strategy | Yes | No | No |\n",
    "\n",
    "##### Metrics notations: dot notation vs multi-dimensional tagged metrics\n",
    "Source: <a href=\"https://sysdig.com/blog/prometheus-metrics/\">https://sysdig.com/blog/prometheus-metrics/</a>\n",
    "For Python we need the third-party library Prometheu to feed the monitoring system.\n",
    "There are two main paradigms to represent a metric: <b>dot notation</b> and <b>multi-dimensional tagged metrics</b>.\n",
    "In a dot-notated metric, data are provided in dot-separated format in the name of the metric, which determine the detail and the hierarchy needed. The arrangement of the metric depends on the piece of information needed. \n",
    "In Prometheus metric format a flat approach is adopted to metrics naming. Instead of a hierarchical, dot separated name, there are names combined with a series of labels or tags. \"Highly dimensional data\" imply the possibility to associate any number of context-specific labels to every submitted metric.\n",
    "\n",
    "##### Prometheus metrics (OpenMetrics)\n",
    "Prometheus metrics format is line oriented: lines are separated by a line feed character (n), and the last line must end with a line feed character, while empty lines are somply ignored.\n",
    "\n",
    "A metric is composed by the (optional) components below:\n",
    "\n",
    "<ul>\n",
    "    <li>Metric name</li>\n",
    "    <li>Any number of labels (can be 0), represented as a key-value array</li>\n",
    "    <li>Current metric value</li>\n",
    "    <li>Optional metric timestamp</li>\n",
    "</ul>\n",
    "\n",
    "Metric output is typically preceded with **# HELP** and **# TYPE** metadata lines.\n",
    "The HELP string identifies the metric name and a brief description of it. The TYPE string identifies the type of metric. If there’s no TYPE before a metric, the metric is set to untyped. Everything else that starts with a # is parsed as a comment.\n",
    "\n",
    "(**continua*)\n",
    "\n",
    "##### Implement Prometheus custom metric instrumentation in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02/03 - 09/03 (OC Index Software Code Refactoring - NOCI + Mapping Merge) <a class=\"anchor\" id=\"entry_2\"></a>\n",
    "\n",
    "### NOCI material\n",
    "\n",
    "1. **OpenCitations Index espansion**\n",
    "\n",
    "   1.1 *ADDITIONS*\n",
    "    \n",
    "      1.1.1 [NIH citation source](#en_1.1.1)\n",
    "\n",
    "      1.1.2 [NOCI glob](#en_1.1.2)\n",
    "\n",
    "      1.1.3 [PMID manager](#en_1.1.3)\n",
    "\n",
    "      1.1.4 [NIH resource finder](#en_1.1.4)\n",
    "        \n",
    "   1.2 *ADJUSTMENTS / EXPANSIONS*\n",
    "    \n",
    "      1.2.1 [citation/oci.py](#en_1.2.1)\n",
    "        \n",
    "      1.2.2 [finder/resourcefinder.py](#en_1.2.2)\n",
    "        \n",
    "      1.2.3 [finder/dataciteresourcefinder.py](#en_1.2.3)\n",
    "      \n",
    "      1.2.4 [finder/crossrefresourcefinder.py](#en_1.2.4)\n",
    "      \n",
    "      1.2.5 [finder/orcidresourcefinder.py](#en_1.2.5)\n",
    "      \n",
    "      1.2.6 [storer/csvmanager.py](#en_1.2.6)\n",
    "      \n",
    "      1.2.7 [storer/datahandler.py](#en_1.2.7)\n",
    "      \n",
    "      1.2.8 [storer/update.py](#en_1.2.8)\n",
    "      \n",
    "      1.2.9 [cnc.py](#en_1.2.9)\n",
    "      \n",
    "      \n",
    "        \n",
    "2. **Ramose**\n",
    "\n",
    "    2.1 [NOCI configuration file](#en_2.1)\n",
    "    \n",
    "    2.2 [indexapi.py extension](#en_2.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 NIH citation source <a class=\"anchor\" id=\"en_1.1.1\"></a>\n",
    "\n",
    "Codice della citation source per il National Institute of Health. Il dataset citazionale (NIH-OCC) fornisce solo le informazioni minime richieste dall'OpenCitations data model, ovvero **citante** e **citato**, espressi rispettivamente nei campi \"citing\" e \"referenced\" del file CSV del NIH-OCC.\n",
    "Il codice è stato testato con successo. \n",
    "Di seguito, il codice del NIH citation source e del rispettivo test case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk, sep, remove\n",
    "from os.path import isdir\n",
    "from json import load\n",
    "from csv import DictWriter\n",
    "from index.citation.citationsource import CSVFileCitationSource\n",
    "from index.identifier.pmidmanager import PMIDManager\n",
    "from index.citation.oci import Citation, OCIManager\n",
    "\n",
    "\n",
    "class NIHCitationSource( CSVFileCitationSource ):\n",
    "    def __init__(self, src, local_name=\"\"):\n",
    "        self.pmid = PMIDManager()\n",
    "        super( NIHCitationSource, self ).__init__( src, local_name )\n",
    "\n",
    "    def get_next_citation_data(self):\n",
    "        row = self._get_next_in_file()\n",
    "        #id_type = OCIManager.pmid_type\n",
    "\n",
    "        while row is not None:\n",
    "            citing = self.pmid.normalise(row.get(\"citing\"))\n",
    "            cited = self.pmid.normalise(row.get(\"referenced\"))\n",
    "\n",
    "            self.update_status_file()\n",
    "            return citing, cited, None, None, None, None #, id_type\n",
    "            self.update_status_file()\n",
    "            row = self._get_next_in_file()\n",
    "\n",
    "        remove(self.status_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test/10_noci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from index.coci.glob import process\n",
    "from os import sep, makedirs\n",
    "from os.path import exists\n",
    "from shutil import rmtree\n",
    "from index.storer.csvmanager import CSVManager\n",
    "from index.noci.nationalinstituteofhealthsource import NIHCitationSource\n",
    "from csv import DictReader\n",
    "\n",
    "\n",
    "class NOCITest(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.input_file = \"index%stest_data%snih_dump%ssource.csv\" % (sep, sep, sep)\n",
    "        self.citations = \"index%stest_data%snih_dump%scitations.csv\" % (sep, sep, sep)\n",
    "\n",
    "    def test_citation_source(self):\n",
    "        ns = NIHCitationSource( self.input_file )\n",
    "        new = []\n",
    "        cit = ns.get_next_citation_data()\n",
    "        while cit is not None:\n",
    "            citing, cited, creation, timespan, journal_sc, author_sc = cit\n",
    "            new.append({\n",
    "                \"citing\": citing,\n",
    "                \"cited\": cited,\n",
    "                \"creation\": \"\" if creation is None else creation,\n",
    "                \"timespan\": \"\" if timespan is None else timespan,\n",
    "                \"journal_sc\": \"no\" if journal_sc is None else journal_sc,\n",
    "                \"author_sc\": \"no\" if author_sc is None else author_sc\n",
    "            })\n",
    "            cit = ns.get_next_citation_data()\n",
    "\n",
    "        with open(self.citations, encoding=\"utf8\") as f:\n",
    "            old = list(DictReader(f))\n",
    "\n",
    "        self.assertEqual(new, old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 NOCI glob <a class=\"anchor\" id=\"en_1.1.2\"></a>\n",
    "L' iCite Database contenente il NIH-OCC, ovvero la data source di NOCI, e un altro dataset: **iCite Metadata**. Se il NIH-OCC contiene solamente dati citazionali rappresentati dai PMID del citante e del citato, iCite Metadata contiene metadati relativi alle entità bibliografiche (identificate da PMID) coinvolte nelle citazioni contenute nel NIH-OCC. iCite Metadata contiene dunque delle informazioni che possono essere rielaborate al fine di ricavarne (direttamente o indirettamente) i metadati utili a completare i quattro campi della tupla a sei elementi non coperti dal NIH-OCC.\n",
    "Tra i campi del dataset iCite Metadata, **\"doi\"** fornisce l'informazione di **mapping** PMID-DOI. Questo dato è particolarmente utile perché permette di sfruttare i servizi API dei DOI per ricavare le informazioni che non vengono fornite né in iCite Metadata né nei servizi API dei PMIDs. \n",
    "I dati ricavati sono salvati in files CSV che vengono utilizzati come materiale di supporto nel processo di popolazione dell'Indice citazionale. \n",
    "Di seguito, il codice del glob di NOCI e il relativo test, passato con successo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from argparse import ArgumentParser\n",
    "from index.storer.csvmanager import CSVManager\n",
    "from index.finder.crossrefresourcefinder import CrossrefResourceFinder\n",
    "from index.finder.orcidresourcefinder import ORCIDResourceFinder\n",
    "from index.identifier.pmidmanager import PMIDManager\n",
    "from index.identifier.doimanager import DOIManager\n",
    "from index.identifier.issnmanager import ISSNManager\n",
    "from index.identifier.orcidmanager import ORCIDManager\n",
    "from os import sep, makedirs, walk\n",
    "import os\n",
    "from os.path import exists\n",
    "import json\n",
    "from re import sub\n",
    "from index.citation.oci import Citation\n",
    "from zipfile import ZipFile\n",
    "from tarfile import TarFile\n",
    "import re\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def issn_data_recover(directory):\n",
    "    journal_issn_dict = dict()\n",
    "    filename = directory + sep + 'journal_issn.json'\n",
    "    if not os.path.exists(filename):\n",
    "        return journal_issn_dict\n",
    "    else:\n",
    "        with open(filename, 'r', encoding='utf8') as fd:\n",
    "            journal_issn_dict = json.load(fd)\n",
    "            types = type(journal_issn_dict)\n",
    "            return journal_issn_dict\n",
    "\n",
    "def issn_data_to_cache(name_issn_dict, directory):\n",
    "    filename = directory + sep + 'journal_issn.json'\n",
    "    with open(filename, 'w', encoding='utf-8' ) as fd:\n",
    "            json.dump(name_issn_dict, fd, ensure_ascii=False, indent=4)\n",
    "\n",
    "#PUB DATE EXTRACTION : takes in input a data structure representing a bibliographic entity\n",
    "def build_pubdate(row):\n",
    "    year = str(row[\"year\"])\n",
    "    str_year = sub( \"[^\\d]\", \"\", year)[:4]\n",
    "    if str_year:\n",
    "        return str_year\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# get_all_files extracts all the needed files from the input directory\n",
    "def get_all_files(i_dir):\n",
    "    result = []\n",
    "    opener = None\n",
    "\n",
    "    if i_dir.endswith( \".zip\" ):\n",
    "        zf = ZipFile( i_dir )\n",
    "        for name in zf.namelist():\n",
    "            if name.lower().endswith(\".csv\") and \"citations\" not in name.lower() and \"source\" not in name.lower():\n",
    "                result.append( name )\n",
    "        opener = zf.open\n",
    "    elif i_dir.endswith( \".tar.gz\" ):\n",
    "        tf = TarFile.open( i_dir )\n",
    "        for name in tf.getnames():\n",
    "            if name.lower().endswith(\".csv\") and \"citations\" not in name.lower() and \"source\" not in name.lower():\n",
    "                result.append(name)\n",
    "        opener = tf.extractfile\n",
    "\n",
    "    else:\n",
    "        for cur_dir, cur_subdir, cur_files in walk(i_dir):\n",
    "            for file in cur_files:\n",
    "                if file.lower().endswith( \".csv\" ) and \"citations\" not in file.lower() and \"source\" not in file.lower():\n",
    "                    result.append(cur_dir + sep + file)\n",
    "        opener = open\n",
    "    return result, opener\n",
    "\n",
    "\n",
    "def process(input_dir, output_dir, n):\n",
    "    if not exists(output_dir):\n",
    "        makedirs(output_dir)\n",
    "\n",
    "    citing_pmid_with_no_date = set()\n",
    "    valid_pmid = CSVManager( output_dir + sep + \"valid_pmid.csv\" )\n",
    "    valid_doi = CSVManager(\"index/test_data/crossref_glob\" + sep + \"valid_doi.csv\")\n",
    "    id_date = CSVManager( output_dir + sep + \"id_date_pmid.csv\" )\n",
    "    id_issn = CSVManager( output_dir + sep + \"id_issn_pmid.csv\" )\n",
    "    id_orcid = CSVManager( output_dir + sep + \"id_orcid_pmid.csv\" )\n",
    "    journal_issn_dict = issn_data_recover(output_dir) #just an empty dict, in case of a code break\n",
    "    pmid_manager = PMIDManager(valid_pmid)\n",
    "    crossref_resource_finder = CrossrefResourceFinder(valid_doi)\n",
    "    orcid_resource_finder = ORCIDResourceFinder(valid_doi)\n",
    "\n",
    "    doi_manager = DOIManager(valid_doi)\n",
    "    issn_manager = ISSNManager()\n",
    "    orcid_manager = ORCIDManager()\n",
    "\n",
    "    all_files, opener = get_all_files(input_dir)\n",
    "    len_all_files = len(all_files)\n",
    "\n",
    "    # Read all the CSV file in the NIH dump to create the main information of all the indexes\n",
    "    print( \"\\n\\n# Add valid PMIDs from NIH metadata\" )\n",
    "    for file_idx, file in enumerate( all_files, 1 ):\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for chunk in pd.read_csv(file, chunksize=1000 ):\n",
    "            f = pd.concat( [df, chunk], ignore_index=True )\n",
    "            f.fillna(\"\", inplace=True)\n",
    "\n",
    "            print( \"Open file %s of %s\" % (file_idx, len_all_files) )\n",
    "            for index, row in f.iterrows():\n",
    "                if int(index) !=0 and int(index) % int(n) == 0:\n",
    "                    print( \"Group nr.\", int(index)//int(n), \"processed. Data from\", int(index), \"rows saved to journal_issn.json mapping file\")\n",
    "                    issn_data_to_cache(journal_issn_dict, output_dir)\n",
    "\n",
    "                citing_pmid = pmid_manager.normalise(row['pmid'], True)\n",
    "                pmid_manager.set_valid(citing_pmid)\n",
    "                citing_doi = doi_manager.normalise(row['doi'], True)\n",
    "\n",
    "                if id_date.get_value(citing_pmid) is None:\n",
    "                    citing_date = Citation.check_date(build_pubdate(row))\n",
    "                    if citing_date is not None:\n",
    "                        id_date.add_value(citing_pmid, citing_date)\n",
    "                        if citing_pmid in citing_pmid_with_no_date:\n",
    "                            citing_pmid_with_no_date.remove(citing_pmid)\n",
    "                    else:\n",
    "                        citing_pmid_with_no_date.add( citing_pmid )\n",
    "\n",
    "                if id_issn.get_value( citing_pmid ) is None:\n",
    "                    journal_name = row[\"journal\"]\n",
    "                    if journal_name: #check that the string is not empty\n",
    "                        if journal_name in journal_issn_dict.keys():\n",
    "                            for issn in journal_issn_dict[journal_name]:\n",
    "                                id_issn.add_value(citing_pmid, issn)\n",
    "                        else:\n",
    "                            if citing_doi is not None:\n",
    "                                json_res = crossref_resource_finder._call_api(citing_doi)\n",
    "                                if json_res is not None:\n",
    "                                    issn_set = crossref_resource_finder._get_issn(json_res)\n",
    "                                    if len(issn_set)>0:\n",
    "                                        journal_issn_dict[journal_name] = []\n",
    "                                    for issn in issn_set:\n",
    "                                        issn_norm = issn_manager.normalise(str(issn))\n",
    "                                        id_issn.add_value( citing_pmid, issn_norm )\n",
    "                                        journal_issn_dict[journal_name].append(issn_norm)\n",
    "\n",
    "\n",
    "                if id_orcid.get_value(citing_pmid) is None:\n",
    "                    if citing_doi is not None:\n",
    "                        json_res = orcid_resource_finder._call_api(citing_doi)\n",
    "                        if json_res is not None:\n",
    "                            orcid_set = orcid_resource_finder._get_orcid(json_res)\n",
    "                            for orcid in orcid_set:\n",
    "                                orcid_norm = orcid_manager.normalise( orcid )\n",
    "                                id_orcid.add_value(citing_pmid, orcid_norm)\n",
    "\n",
    "            issn_data_to_cache( journal_issn_dict, output_dir )\n",
    "\n",
    "\n",
    "    # Iterate once again for all the rows of all the csv files, so to check the validity of the referenced pmids.\n",
    "    print( \"\\n\\n# Checking the referenced pmids validity\" )\n",
    "    for file_idx, file in enumerate( all_files, 1 ):\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for chunk in pd.read_csv( file, chunksize=1000 ):\n",
    "            f = pd.concat( [df, chunk], ignore_index=True )\n",
    "            f.fillna(\"\", inplace=True)\n",
    "            print( \"Open file %s of %s\" % (file_idx, len_all_files) )\n",
    "            for index, row in f.iterrows():\n",
    "                if row[\"references\"] != \"\":\n",
    "                    ref_string = row[\"references\"].strip()\n",
    "                    ref_string_norm = re.sub(\"\\s+\", \" \", ref_string)\n",
    "                else:\n",
    "                    print(\"the type of row reference is\", (row[\"references\"]), type(row[\"references\"]))\n",
    "                    print(index, row )\n",
    "\n",
    "                cited_pmids = set(ref_string_norm.split(\" \"))\n",
    "                for cited_pmid in cited_pmids:\n",
    "                    if pmid_manager.is_valid(cited_pmid):\n",
    "                        print(\"valid cited pmid added:\", cited_pmid)\n",
    "                    else:\n",
    "                        print(\"invalid cited pmid discarded:\", cited_pmid)\n",
    "\n",
    "    for pmid in citing_pmid_with_no_date:\n",
    "        id_date.add_value( pmid, \"\" )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    arg_parser = ArgumentParser( \"Global files creator for NOCI\",\n",
    "                                 description=\"Process NIH CSV files and create global indexes to enable \"\n",
    "                                             \"the creation of NOCI.\" )\n",
    "    arg_parser.add_argument( \"-i\", \"--input_dir\", dest=\"input_dir\", required=True,\n",
    "                             help=\"Either the directory or the zip file that contains the NIH data dump \"\n",
    "                                  \"of CSV files.\" )\n",
    "    arg_parser.add_argument( \"-o\", \"--output_dir\", dest=\"output_dir\", required=True,\n",
    "                             help=\"The directory where the indexes are stored.\" )\n",
    "\n",
    "\n",
    "    arg_parser.add_argument( \"-n\", \"--num_lines\", dest=\"n\", required=True,\n",
    "                             help=\"Number of lines after which the data stored in the dictionary for the mapping \"\n",
    "                                  \"between a Journal name and the related issns are passed into a JSON cache file\" )\n",
    "\n",
    "\n",
    "    args = arg_parser.parse_args()\n",
    "\n",
    "    start = timer()\n",
    "    process(args.input_dir, args.output_dir, args.n)\n",
    "    end = timer()\n",
    "    #calculate elapsed time\n",
    "    print(\"elapsed time, in seconds:\", (end-start))\n",
    "\n",
    "\n",
    "#python -m index.noci.glob1 -i \"index/test_data/nih_dump\" -o \"index/test_data/nih_glob1\" -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test/13_glob1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from os import sep, remove\n",
    "import os\n",
    "from os.path import exists\n",
    "from index.noci.glob1 import issn_data_recover, issn_data_to_cache, build_pubdate, get_all_files, process\n",
    "from index.storer.csvmanager import CSVManager\n",
    "from index.identifier.issnmanager import ISSNManager\n",
    "from index.identifier.orcidmanager import ORCIDManager\n",
    "from index.identifier.pmidmanager import PMIDManager\n",
    "from index.identifier.doimanager import DOIManager\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "class MyTestCase( unittest.TestCase ):\n",
    "    def setUp(self):\n",
    "        self.dir_with_issn_map = \"index%stest_data%sglob_noci%sissn_data_recover%swith_issn_mapping\" % (sep, sep, sep, sep)\n",
    "        self.dir_without_issn_map = \"index%stest_data%sglob_noci%sissn_data_recover%swithout_issn_mapping\" % (sep, sep, sep, sep)\n",
    "        self.issn_journal_sample_dict = {\"N Biotechnol\": [\"1871-6784\"], \"Biochem Med\": [\"0006-2944\"], \"Magn Reson Chem\": [\"0749-1581\"]}\n",
    "        self.data_to_cache_dir = \"index%stest_data%sglob_noci%sissn_data_to_cache\" % (sep, sep, sep)\n",
    "        self.get_all_files_dir = \"index%stest_data%sglob_noci%sget_all_files\" % (sep, sep, sep)\n",
    "        self.csv_sample = \"index%stest_data%sglob_noci%sget_all_files%s1.csv\" % (sep, sep, sep, sep)\n",
    "        self.output_dir = \"index%stest_data%sglob_noci%sprocess%soutput\" % (sep, sep, sep, sep)\n",
    "        self.valid_pmid = CSVManager( self.output_dir + sep + \"valid_pmid.csv\" )\n",
    "        self.valid_doi = CSVManager( \"index/test_data/crossref_glob\" + sep + \"valid_doi.csv\" )\n",
    "        self.id_date = CSVManager( self.output_dir + sep + \"id_date_pmid.csv\" )\n",
    "        self.id_issn = CSVManager( self.output_dir + sep + \"id_issn_pmid.csv\" )\n",
    "        self.id_orcid = CSVManager( self.output_dir + sep + \"id_orcid_pmid.csv\" )\n",
    "        self.doi_manager = DOIManager(self.valid_doi)\n",
    "        self.pmid_manager = PMIDManager(self.valid_pmid)\n",
    "        self.issn_manager = ISSNManager()\n",
    "        self.orcid_manager = ORCIDManager()\n",
    "        self.sample_reference = \"pmid:7829625\"\n",
    "\n",
    "    def test_issn_data_recover(self):\n",
    "        #Test the case in which there is no mapping file for journals - issn\n",
    "        self.assertEqual(issn_data_recover(self.dir_without_issn_map), {})\n",
    "        #Test the case in which there is a mapping file for journals - issn\n",
    "        issn_map_dict_len = len(issn_data_recover(self.dir_with_issn_map))\n",
    "        self.assertTrue(issn_map_dict_len>0)\n",
    "\n",
    "    def test_issn_data_to_cache(self):\n",
    "        filename = self.data_to_cache_dir + sep + 'journal_issn.json'\n",
    "        if exists(filename):\n",
    "            remove(filename)\n",
    "        self.assertFalse(exists(filename))\n",
    "        issn_data_to_cache(self.issn_journal_sample_dict, self.data_to_cache_dir)\n",
    "        self.assertTrue(exists(filename))\n",
    "\n",
    "    def test_get_all_files(self):\n",
    "        all_files, opener = get_all_files( self.get_all_files_dir)\n",
    "        len_all_files = len(all_files)\n",
    "        #The folder contains 4 csv files, but two of those contains the words \"citations\" or \"source\" in their filenames\n",
    "        self.assertEqual( len_all_files, 2)\n",
    "\n",
    "    def test_build_pubdate(self):\n",
    "        df = pd.DataFrame()\n",
    "        for chunk in pd.read_csv(self.csv_sample, chunksize=1000):\n",
    "            f = pd.concat( [df, chunk], ignore_index=True )\n",
    "            f.fillna( \"\", inplace=True )\n",
    "            for index, row in f.iterrows():\n",
    "                pub_date = build_pubdate(row)\n",
    "                self.assertTrue(isinstance(pub_date, str))\n",
    "                self.assertTrue(isinstance(int(pub_date), int))\n",
    "                self.assertEqual(len(pub_date), 4)\n",
    "\n",
    "    def test_process(self):\n",
    "        for files in os.listdir( self.output_dir):\n",
    "            path = os.path.join( self.output_dir, files )\n",
    "            try:\n",
    "                shutil.rmtree(path)\n",
    "            except OSError:\n",
    "                os.remove(path)\n",
    "        self.assertEqual(len(os.listdir(self.output_dir)),0)\n",
    "        process(self.get_all_files_dir, self.output_dir, 20)\n",
    "        self.assertTrue(len(os.listdir(self.output_dir))>0)\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for chunk in pd.read_csv( self.csv_sample, chunksize=1000 ):\n",
    "            f = pd.concat( [df, chunk], ignore_index=True )\n",
    "            f.fillna( \"\", inplace=True )\n",
    "            for index, row in f.iterrows():\n",
    "                if index == 1:\n",
    "                    pmid = row[\"pmid\"]\n",
    "\n",
    "        citing_pmid = self.pmid_manager.normalise(pmid, include_prefix=True)\n",
    "\n",
    "        self.assertEqual(self.valid_pmid.get_value(citing_pmid), {'v'})\n",
    "        self.assertEqual(self.valid_pmid.get_value(self.sample_reference), {'v'})\n",
    "        self.assertEqual(self.id_date.get_value(citing_pmid), {'1998'})\n",
    "        self.assertEqual(self.id_issn.get_value(citing_pmid), {'0918-8959', '1348-4540'})\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for chunk in pd.read_csv( self.csv_sample, chunksize=1000 ):\n",
    "            f = pd.concat( [df, chunk], ignore_index=True )\n",
    "            f.fillna( \"\", inplace=True )\n",
    "            for index, row in f.iterrows():\n",
    "                if index == 0:\n",
    "                    pmid = row[\"pmid\"]\n",
    "\n",
    "        citing_pmid = self.pmid_manager.normalise(pmid, include_prefix=True)\n",
    "\n",
    "        self.assertEqual(self.id_orcid.get_value(citing_pmid), {'0000-0002-0524-4077'})\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "\n",
    "#python -m unittest index.test.13_glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 PMID manager <a class=\"anchor\" id=\"en_1.1.3\"></a>\n",
    "La classe PMIDManager è sviluppata come **istanza della superclasse IdentifierManager**. Lo sviluppo del PMIDManager è modellato sull'esempio della classe DOIManager, con cui condivide scopo e funzioni.\n",
    "In particolare, gli identifier manager si occupano di normalizzare il formato degli identificativi, per poi verificarne l'esistenza e la validità ricorrendo a servizi di API specifici per ogni tipo di identificativo.\n",
    "L'**API service** utilizzato per i PMID è **https://pubmed.ncbi.nlm.nih.gov/**, che fornisce in risposta una **pagina HTML**. Per questo motivo, l'informazione relativa all'avvenuta o mancata validazione del PMID in questione viene estratta con gli strumenti forniti dalla libreria **BeautifulSoup**. \n",
    "Di seguito, il codice del PMID manager e l'estensione del test case per gli identifier managers. Il test è stato passato con successo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from index.identifier.identifiermanager import IdentifierManager\n",
    "from re import sub, match\n",
    "from urllib.parse import unquote, quote\n",
    "from requests import get\n",
    "from index.storer.csvmanager import CSVManager\n",
    "from requests import ReadTimeout\n",
    "from requests.exceptions import ConnectionError\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "class PMIDManager( IdentifierManager ):\n",
    "    def __init__(self, valid_pmid=None, use_api_service=True):\n",
    "        if valid_pmid is None:\n",
    "            valid_pmid = CSVManager( store_new=False )\n",
    "\n",
    "        self.api = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
    "        self.valid_pmid = valid_pmid\n",
    "        self.use_api_service = use_api_service\n",
    "        self.p = \"pmid:\"\n",
    "        super( PMIDManager, self ).__init__()\n",
    "\n",
    "    def set_valid(self, id_string):\n",
    "        pmid = self.normalise(id_string, include_prefix=True )\n",
    "        if self.valid_pmid.get_value( pmid ) is None:\n",
    "            self.valid_pmid.add_value( pmid, \"v\" )\n",
    "\n",
    "    def is_valid(self, id_string):\n",
    "        pmid = self.normalise( id_string, include_prefix=True )\n",
    "        if pmid is None or match( \"^pmid:[1-9]\\d*$\", pmid ) is None:\n",
    "            return False\n",
    "        else:\n",
    "            if self.valid_pmid.get_value( pmid ) is None:\n",
    "                if self.__pmid_exists( pmid ):\n",
    "                    self.valid_pmid.add_value( pmid, \"v\" )\n",
    "                else:\n",
    "                    self.valid_pmid.add_value( pmid, \"i\" )\n",
    "            return \"v\" in self.valid_pmid.get_value( pmid )\n",
    "\n",
    "    def normalise(self, id_string, include_prefix=False):\n",
    "        id_string = str(id_string)\n",
    "        try:\n",
    "            pmid_string = sub( \"^0+\", \"\", sub( \"\\0+\", \"\", (sub( \"[^\\d+]\", \"\", id_string )) ) )\n",
    "            return \"%s%s\" % (self.p if include_prefix else \"\", pmid_string)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def __pmid_exists(self, pmid_full):\n",
    "        pmid = self.normalise( pmid_full )\n",
    "        if self.use_api_service:\n",
    "            tentative = 3\n",
    "            while tentative:\n",
    "                tentative -= 1\n",
    "                try:\n",
    "                    r = get( self.api + quote( pmid ) + \"/?format=pmid\", headers=self.headers, timeout=30 )\n",
    "                    if r.status_code == 200:\n",
    "                        r.encoding = \"utf-8\"\n",
    "\n",
    "                        soup = BeautifulSoup( r.content, features=\"lxml\" )\n",
    "                        for i in soup.find_all( \"meta\", {\"name\": \"uid\"} ):\n",
    "                            id = i[\"content\"]\n",
    "                            if id == pmid:\n",
    "                                return True\n",
    "\n",
    "                except ReadTimeout:\n",
    "                    pass\n",
    "                except ConnectionError:\n",
    "                    sleep(5)\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test/02_identifiermanager.py (PMID extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from os import sep\n",
    "from index.identifier.doimanager import DOIManager\n",
    "from index.identifier.issnmanager import ISSNManager\n",
    "from index.identifier.orcidmanager import ORCIDManager\n",
    "from index.identifier.pmidmanager import PMIDManager\n",
    "from index.storer.csvmanager import CSVManager\n",
    "\n",
    "\n",
    "class IdentifierManagerTest(unittest.TestCase):\n",
    "    \"\"\"This class aim at testing the methods of the class CSVManager.\"\"\"\n",
    "\n",
    "    def setUp(self):\n",
    "#[...]\n",
    "\n",
    "#class extension for pubmedid\n",
    "        self.valid_pmid_1 = \"2942070\"\n",
    "        self.valid_pmid_2 = \"1509982\"\n",
    "        self.valid_pmid_3 = \"7189714\"\n",
    "        self.invalid_pmid_1 = \"0067308798798\"\n",
    "        self.invalid_pmid_2 = \"pmid:174777777777\"\n",
    "        self.invalid_pmid_3 = \"000009265465465465\"\n",
    "        self.valid_pmid_path = \"index%stest_data%svalid_pmid.csv\" % (sep, sep)\n",
    "\n",
    "#[...]\n",
    "\n",
    "#class extension for pubmedid\n",
    "    def test_pmid_normalise(self):\n",
    "        pm = PMIDManager()\n",
    "        self.assertEqual(self.valid_pmid_1, pm.normalise(self.valid_pmid_1.replace(\"\", \"pmid:\")))\n",
    "        self.assertEqual(self.valid_pmid_1, pm.normalise(self.valid_pmid_1.replace(\"\", \" \")))\n",
    "        self.assertEqual(self.valid_pmid_1, pm.normalise(\"https://pubmed.ncbi.nlm.nih.gov/\"+self.valid_pmid_1))\n",
    "        self.assertEqual(self.valid_pmid_2, pm.normalise(\"000\"+self.valid_pmid_2))\n",
    "\n",
    "    def test_pmid_is_valid(self):\n",
    "        pm_nofile = PMIDManager()\n",
    "        print(pm_nofile.normalise(self.valid_pmid_1, include_prefix=True ))\n",
    "        print(pm_nofile.is_valid(self.valid_pmid_1))\n",
    "        self.assertTrue(pm_nofile.is_valid(self.valid_pmid_1))\n",
    "        self.assertTrue(pm_nofile.is_valid(self.valid_pmid_2))\n",
    "        self.assertTrue(pm_nofile.is_valid(self.valid_pmid_3))\n",
    "        self.assertFalse(pm_nofile.is_valid(self.invalid_pmid_1))\n",
    "        self.assertFalse(pm_nofile.is_valid(self.invalid_pmid_2))\n",
    "        self.assertFalse(pm_nofile.is_valid(self.invalid_pmid_3))\n",
    "\n",
    "        valid_pmid = CSVManager(self.valid_pmid_path)\n",
    "        pm_file = PMIDManager(valid_pmid=valid_pmid, use_api_service=False)\n",
    "        self.assertTrue(pm_file.is_valid(self.valid_pmid_1))\n",
    "        self.assertFalse(pm_file.is_valid(self.invalid_pmid_1))\n",
    "\n",
    "        pm_nofile_noapi = PMIDManager(use_api_service=False)\n",
    "        self.assertFalse(pm_nofile_noapi.is_valid(self.valid_pmid_1))\n",
    "        self.assertFalse(pm_nofile_noapi.is_valid(self.invalid_pmid_1))\n",
    "        self.assertFalse(pm_nofile_noapi.is_valid(self.valid_pmid_2))\n",
    "        self.assertFalse(pm_nofile_noapi.is_valid(self.invalid_pmid_2))\n",
    "        self.assertFalse(pm_nofile_noapi.is_valid(self.valid_pmid_3))\n",
    "        self.assertFalse(pm_nofile_noapi.is_valid(self.invalid_pmid_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 NIH resource finder <a class=\"anchor\" id=\"en_1.1.4\"></a>\n",
    "La classe NIHResourceFinder è sviluppata come **istanza della superclasse ApiIDResourceFinder** e il suo scopo è quello di **recuperare metadati dall'API per i PMID**, nel caso in cui nel processo non vengano forniti files di supporto generati nel preprocessing dal glob di NOCI. \n",
    "Il servizio utilizzato è https://pubmed.ncbi.nlm.nih.gov/ (con display option \"pubmed\") che restituisce risposte in formato HTML. Per questo motivo, i dati sono estratti utilizzando la libreria **Beautiful Soup** per accedere alla sezione che contiene la stringa testuale con i metadati. A questo punto, i dati sono estratti dalla stringa testuale sfruttando le **regex**. \n",
    "A differenza dei servizi API per i DOI, questa REST API non fornisce dati particolarmente dettagliati e non copre alcune delle informazioni richieste dall'OCDM, come ad esempio gli ORCID degli autori. Tra i metadati a disposizione, gli unici utili sono la **data di pubblicazione** e l'**ISSN** della rivista di pubblicazione. \n",
    "Di seguito, lo script del NIHResourceFinder e la relativa espansionsione del testcase 03_resourcefinder.py. Il test è stato superato con successo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from index.finder.resourcefinder import ApiIDResourceFinder\n",
    "from index.citation.oci import OCIManager\n",
    "from requests import get\n",
    "from urllib.parse import quote\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class NIHResourceFinder(ApiIDResourceFinder):\n",
    "    def __init__(self, date=None, orcid=None, issn=None, pmid=None, use_api_service=True):\n",
    "        self.use_api_service = use_api_service\n",
    "        self.api = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
    "        self.baseurl = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
    "        super(NIHResourceFinder, self).__init__(date=date, orcid=orcid, issn=issn, id=pmid, id_type=OCIManager.pmid_type,\n",
    "                                                     use_api_service=use_api_service)\n",
    "\n",
    "    def _get_issn(self, txt_obj):\n",
    "        result = set()\n",
    "        issns = re.findall(\"IS\\s+-\\s+\\d{4}-\\d{4}\", txt_obj)\n",
    "        for i in issns:\n",
    "            issn = re.search(\"\\d{4}-\\d{4}\", i).group(0)\n",
    "            result.add(issn)\n",
    "        return result\n",
    "\n",
    "    def _get_date(self, txt_obj):\n",
    "        date = re.search(\"DP\\s+-\\s+(\\d{4}(\\s?(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))?(\\s?((3[0-1])|([1-2][0-9])|([0]?[1-9])))?)\", txt_obj).group(1)\n",
    "        re_search = re.search(\"(\\d{4})\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+((3[0-1])|([1-2][0-9])|([0]?[1-9]))\", date)\n",
    "        if re_search is not None:\n",
    "            result = re_search.group(0)\n",
    "            datetime_object = datetime.strptime(result, '%Y %b %d')\n",
    "            return datetime.strftime(datetime_object, '%Y-%m-%d')\n",
    "        else:\n",
    "            re_search = re.search(\"(\\d{4})\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\", date)\n",
    "            if re_search is not None:\n",
    "                result = re_search.group(0)\n",
    "                datetime_object = datetime.strptime(result, '%Y %b')\n",
    "                return datetime.strftime(datetime_object, '%Y-%m')\n",
    "            else:\n",
    "                re_search = re.search(\"(\\d{4})\", date)\n",
    "                if re_search is not None:\n",
    "                    result = re.search(\"(\\d{4})\", date).group(0)\n",
    "                    datetime_object = datetime.strptime(result, '%Y')\n",
    "                    return datetime.strftime(datetime_object, '%Y')\n",
    "                else:\n",
    "                    return None\n",
    "\n",
    "\n",
    "    def _call_api(self, pmid_full):\n",
    "        if self.use_api_service:\n",
    "            pmid = self.pm.normalise(pmid_full)\n",
    "            r = get(self.api + quote(pmid) + \"/?format=pubmed\", headers=self.headers, timeout=30)\n",
    "            if r.status_code == 200:\n",
    "                r.encoding = \"utf-8\"\n",
    "                soup = BeautifulSoup(r.text, features=\"lxml\")\n",
    "                mdata = str(soup.find(id=\"article-details\"))\n",
    "                return mdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test/03_resourcefinder.py (PMID extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from os import sep\n",
    "from index.storer.csvmanager import CSVManager\n",
    "from index.finder.crossrefresourcefinder import CrossrefResourceFinder\n",
    "from index.finder.dataciteresourcefinder import DataCiteResourceFinder\n",
    "from index.finder.nihresourcefinder import NIHResourceFinder\n",
    "from index.finder.orcidresourcefinder import ORCIDResourceFinder\n",
    "from index.finder.resourcefinder import ResourceFinderHandler\n",
    "\n",
    "\n",
    "class ResourceFinderTest(unittest.TestCase):\n",
    "    \"\"\"This class aim at testing the methods of the class CSVManager.\"\"\"\n",
    "\n",
    "    def setUp(self):\n",
    "        self.date_path = \"index%stest_data%sid_date.csv\" % (sep, sep)\n",
    "        self.date_path_pmid = \"index%stest_data%sid_date_pmid.csv\" % (sep, sep)\n",
    "        self.orcid_path = \"index%stest_data%sid_orcid.csv\" % (sep, sep)\n",
    "        self.orcid_path_pmid = \"index%stest_data%sid_orcid_pmid.csv\" % (sep, sep)\n",
    "        self.issn_path = \"index%stest_data%sid_issn.csv\" % (sep, sep)\n",
    "        self.issn_path_pmid = \"index%stest_data%sid_issn_pmid.csv\" % (sep, sep)\n",
    "        self.doi_path = \"index%stest_data%svalid_doi.csv\" % (sep, sep)\n",
    "        self.pmid_path = \"index%stest_data%svalid_pmid.csv\" % (sep, sep)\n",
    "#[...]\n",
    "    \n",
    "    def test_nationalinstititeofhealth_get_orcid(self):\n",
    "        #Do not use support files, only APIs\n",
    "        nf_1 = NIHResourceFinder()\n",
    "        self.assertIsNone(nf_1.get_orcid(\"7189714\"))\n",
    "        self.assertIsNone(nf_1.get_orcid(\"1509982\"))\n",
    "\n",
    "        # Do use support files, but avoid using APIs\n",
    "        nf_2 = NIHResourceFinder(orcid=CSVManager(self.orcid_path_pmid),\n",
    "                                      pmid=CSVManager(self.pmid_path), use_api_service=False)\n",
    "        self.assertIn(\"0000-0002-1825-0097\", nf_2.get_orcid(\"7189714\"))\n",
    "        self.assertNotIn(\"0000-0002-1825-0098\", nf_2.get_orcid(\"1509982\"))\n",
    "\n",
    "        # Do not use support files neither APIs\n",
    "        nf_3 = NIHResourceFinder(use_api_service=False)\n",
    "        self.assertIsNone(nf_3.get_orcid(\"7189714\"))\n",
    "\n",
    "    def test_nationalinstititeofhealth_get_issn(self):\n",
    "        # Do not use support files, only APIs\n",
    "        nf_1 = NIHResourceFinder()\n",
    "        self.assertIn(\"0003-4819\", nf_1.get_container_issn(\"2942070\"))\n",
    "        self.assertNotIn(\"0003-0000\", nf_1.get_container_issn(\"2942070\"))\n",
    "\n",
    "        # # Do use support files, but avoid using APIs\n",
    "        nf_2 = NIHResourceFinder(issn=CSVManager(self.issn_path_pmid),\n",
    "                                      pmid=CSVManager(self.pmid_path), use_api_service=False)\n",
    "        container = nf_2.get_container_issn(\"1509982\")\n",
    "        self.assertIn(\"0065-4299\", container)\n",
    "        self.assertNotIn(\"0065-4444\", nf_2.get_container_issn(\"1509982\"))\n",
    "\n",
    "        # Do not use support files neither APIs\n",
    "        nf_3 = NIHResourceFinder(use_api_service=False)\n",
    "        self.assertIsNone(nf_3.get_container_issn(\"7189714\"))\n",
    "\n",
    "    def test_nationalinstititeofhealth_get_pub_date(self):\n",
    "        # Do not use support files, only APIs\n",
    "        nf_1 = NIHResourceFinder()\n",
    "        self.assertIn(\"1998-05-25\", nf_1.get_pub_date(\"9689714\"))\n",
    "        self.assertNotEqual(\"1998\", nf_1.get_pub_date(\"9689714\"))\n",
    "\n",
    "        # Do not use support files, only APIs\n",
    "        nf_2 = NIHResourceFinder(date=CSVManager(self.date_path_pmid),\n",
    "                                      pmid=CSVManager(self.pmid_path), use_api_service=False)\n",
    "        self.assertIn(\"1980-06\", nf_2.get_pub_date(\"7189714\"))\n",
    "        self.assertNotEqual(\"1980-06-22\", nf_2.get_pub_date(\"7189714\"))\n",
    "\n",
    "        # Do not use support files neither APIs\n",
    "        nf_3 = NIHResourceFinder(use_api_service=False)\n",
    "        self.assertIsNone(nf_3.get_pub_date(\"2942070\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 citation/oci.py <a class=\"anchor\" id=\"en_1.2.1\"></a>\n",
    "Dovendo differenziare il processo a seconda del tipo di identificativo, sono stati introdotti due variabili di classe a OCIManager: **doi_type** and **pmid_type**, i cui valori sono le stringhe “doi” e “pmid”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCIManager(object):\n",
    "    doi_type = \"doi\"\n",
    "    pmid_type = \"pmid\"\n",
    "    def __init__(self, oci_string=None, lookup_file=None, conf_file=None, doi_1=None, doi_2=None, prefix=\"\"):\n",
    "        #[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 finder/resourcefinder.py <a class=\"anchor\" id=\"en_1.2.2\"></a>\n",
    "1) Aggiunta di **id_type tra i parametri del costruttore di ResourceFinder**, così da **differenziare l'identifier manager** coerentemente col tipo di identificativo.\n",
    "2) Differenziazione dei metodi di **normalizzazione e validazione** nella classe **APIIDResourceFinder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResourceFinder(object):\n",
    "    \"\"\"This is the abstract class that must be implemented by any resource finder\n",
    "    for a particular service (Crossref, DataCite, ORCiD, etc.). It provides\n",
    "    the signatures of the methods that should be implemented, and a basic\n",
    "    constructor.\"\"\"\n",
    "\n",
    "    def __init__(self, date=None, orcid=None, issn=None, id=None, id_type=None, **params):\n",
    "        if date is None:\n",
    "            date = CSVManager(store_new=False)\n",
    "        if orcid is None:\n",
    "            orcid = CSVManager(store_new=False)\n",
    "        if issn is None:\n",
    "            issn = CSVManager(store_new=False)\n",
    "        if id is None:\n",
    "            id = CSVManager(store_new=False)\n",
    "\n",
    "        for key in params:\n",
    "            setattr(self, key, params[key])\n",
    "\n",
    "        self.issn = issn\n",
    "        self.date = date\n",
    "        self.orcid = orcid\n",
    "        self.id_type = id_type\n",
    "        if hasattr(self, 'use_api_service'):\n",
    "            if id_type is OCIManager.doi_type:\n",
    "                print(id.csv_path)  # None\n",
    "                self.dm = DOIManager(id, self.use_api_service)\n",
    "            elif id_type is OCIManager.pmid_type:\n",
    "                self.pm = PMIDManager(id, self.use_api_service)\n",
    "            else:\n",
    "                print(\"The id_type specified is not compliant\")\n",
    "        else:\n",
    "            if id_type is OCIManager.doi_type:\n",
    "                self.dm = DOIManager(id)\n",
    "            elif id_type is OCIManager.pmid_type:\n",
    "                self.pm = PMIDManager(id)\n",
    "            else:\n",
    "                print(\"The id_type specified is not compliant\")\n",
    "\n",
    "        self.im = ISSNManager()\n",
    "        self.om = ORCIDManager()\n",
    "\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"ResourceFinder / OpenCitations Indexes \"\n",
    "                          \"(http://opencitations.net; mailto:contact@opencitations.net)\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApiIDResourceFinder(ResourceFinder): #The name of the class was changed\n",
    "    \"\"\"This is the abstract class that must be implemented by any resource finder\n",
    "        for a particular service which is based on DOI retrieving via HTTP REST APIs\n",
    "        (Crossref, DataCite). It provides basic methods that are be used for\n",
    "        implementing the main methods of the ResourceFinder abstract class.\"\"\"\n",
    "\n",
    "    # The following four methods are those ones that should be implemented in\n",
    "    # the concrete subclasses of this abstract class.\n",
    "    def _get_date(self, json_obj):\n",
    "        pass\n",
    "\n",
    "    def _get_issn(self, json_obj):\n",
    "        return set()\n",
    "\n",
    "    def _get_orcid(self, json_obj):\n",
    "        return set()\n",
    "\n",
    "    def _call_api(self, id_full):\n",
    "        pass\n",
    "\n",
    "    def get_orcid(self, id_string):\n",
    "        return self._get_item(id_string, self.orcid)\n",
    "\n",
    "    def get_pub_date(self, id_string):\n",
    "        return self._get_item(id_string, self.date)\n",
    "\n",
    "    def get_container_issn(self, id_string):\n",
    "        return self._get_item(id_string, self.issn)\n",
    "\n",
    "    def is_valid(self, id_string):\n",
    "        if self.id_type == OCIManager.doi_type:\n",
    "            return self.dm.is_valid(id_string)\n",
    "        elif self.id_type == OCIManager.pmid_type:\n",
    "            return self.pm.is_valid(id_string)\n",
    "        else:\n",
    "            print(\"The id_type specified is not compliant\")\n",
    "\n",
    "    def normalise(self, id_string):\n",
    "        if self.id_type is OCIManager.doi_type:\n",
    "            return self.dm.normalise(id_string, include_prefix=True)\n",
    "        elif self.id_type is OCIManager.pmid_type:\n",
    "            return self.pm.normalise(id_string, include_prefix=True)\n",
    "        else:\n",
    "            print(\"The id_type specified is not compliant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 finder/dataciteresourcefinder.py <a class=\"anchor\" id=\"en_1.2.3\"></a>\n",
    "Conseguentemente alle modifiche in ApiIDResource finder, i **parametri dei costruttori di classe delle istanze di ApiIDresourceeFinder sono stati modificati**. \n",
    "Nello specifico, doi=doi è stato modificato in id=doi, ed è stato aggiunto id_type=OCIManager.doi_type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCiteResourceFinder(ApiIDResourceFinder):\n",
    "    def __init__(self, date=None, orcid=None, issn=None, doi=None, use_api_service=True):\n",
    "        self.api = \"https://api.datacite.org/dois/\"\n",
    "        self.use_api_service = use_api_service\n",
    "        super(DataCiteResourceFinder, self).__init__(date=date, orcid=orcid, issn=issn, id=doi, id_type=OCIManager.doi_type,\n",
    "                                                     use_api_service=use_api_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 finder/crossrefresourcefinder.pyr <a class=\"anchor\" id=\"en_1.2.4\"></a>\n",
    "Conseguentemente alle modifiche in ApiIDResource finder, i **parametri dei costruttori di classe delle istanze di ApiIDresourceeFinder sono stati modificati**. \n",
    "Nello specifico, doi=doi è stato modificato in id=doi, ed è stato aggiunto id_type=OCIManager.doi_type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossrefResourceFinder(ApiIDResourceFinder):\n",
    "    def __init__(self, date=None, orcid=None, issn=None, doi=None, use_api_service=True):\n",
    "        self.use_api_service = use_api_service\n",
    "        self.api = \"https://api.crossref.org/works/\"\n",
    "        super(CrossrefResourceFinder, self).__init__(date=date, orcid=orcid, issn=issn, id=doi, id_type=OCIManager.doi_type,\n",
    "                                                     use_api_service=use_api_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 finder/orcidresourcefinder.py <a class=\"anchor\" id=\"en_1.2.5\"></a>\n",
    "Conseguentemente alle modifiche in ApiIDResource finder, i **parametri dei costruttori di classe delle istanze di ApiIDresourceeFinder sono stati modificati**. \n",
    "Nello specifico, doi=doi è stato modificato in id=doi, ed è stato aggiunto id_type=OCIManager.doi_type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORCIDResourceFinder(ApiIDResourceFinder):\n",
    "    def __init__(self, date=None, orcid=None, issn=None, doi=None, use_api_service=True, key=None):\n",
    "        self.key = key\n",
    "        self.use_api_service = use_api_service\n",
    "        self.api = \"https://pub.orcid.org/v2.1/search?q=\"\n",
    "        super(ORCIDResourceFinder, self).__init__(date=date, orcid=orcid, issn=issn, id=doi, id_type=OCIManager.doi_type,\n",
    "                                                  use_api_service=use_api_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 storer/csvmanager.py <a class=\"anchor\" id=\"en_1.2.6\"></a>\n",
    "Nuovo metodo aggiunto a csv manager, per sostituire il valore associato ad un identificativo, nella colonna \"value\" di un file csv gestito dal csv manager. \n",
    "*Probabilmente non da aggiungere nel merge, perché pensato per il mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_value(self, id_string, value):\n",
    "    \"\"\"Substitute the value of a csv line if the value associated with the id is not the expected one\"\"\"\n",
    "    if id_string in self.data and str( value ) not in self.data[id_string]:\n",
    "        self.data[id_string].clear()\n",
    "        self.data[id_string].add( str( value ) )\n",
    "        filename = self.csv_path\n",
    "        tempfile = NamedTemporaryFile(mode='w', newline='', delete=False)\n",
    "        fields = [\"id\", \"value\"]\n",
    "        with open( filename, 'r', encoding=\"utf8\", newline='') as csvfile, tempfile:\n",
    "            reader = csv.DictReader(csvfile, fieldnames=fields)\n",
    "            writer = csv.DictWriter(tempfile, fieldnames=fields)\n",
    "            for row in reader:\n",
    "                if row[\"id\"] == id_string and str(value) != row[\"value\"]:\n",
    "                    print( 'updating row', row['id'] )\n",
    "                    row[\"id\"], row[\"value\"] = id_string, str(value)\n",
    "                row = {\"id\": row[\"id\"], \"value\": row[\"value\"]}\n",
    "                writer.writerow(row)\n",
    "        shutil.move( tempfile.name, filename )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.7 storer/datahandler.py <a class=\"anchor\" id=\"en_1.2.7\"></a>\n",
    "1) Aggiunta di **\"noci\" (NIHCitationSource) tra le source classess del DataHandler** (vedi parametri di cnc)\n",
    "\n",
    "2) Aggiunta del parametro **id_type nel costruttore di FileDataHandler**, così da poter **attribuire i possibili resource finders in relazione al tipo di identificativo specificato** (crossref, datactie e orcid nel caso del DOI e NIH nel caso del PMID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler(object):\n",
    "    \"\"\"A class acting as a proxy for accessing specific data useful to create citations.\"\"\"\n",
    "    _source_classes = {\n",
    "        \"csv\": CSVFileCitationSource,\n",
    "        \"crossref\": CrossrefCitationSource,\n",
    "        \"croci\": CrowdsourcedCitationSource,\n",
    "        \"noci\" : NIHCitationSource,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileDataHandler(DataHandler):\n",
    "    @staticmethod\n",
    "    def _create_csv(doi_file, date_file, orcid_file, issn_file):\n",
    "        valid_doi = CSVManager(csv_path=doi_file)\n",
    "        id_date = CSVManager(csv_path=date_file)\n",
    "        id_orcid = CSVManager(csv_path=orcid_file)\n",
    "        id_issn = CSVManager(csv_path=issn_file)\n",
    "\n",
    "        return valid_doi, id_date, id_orcid, id_issn\n",
    "\n",
    "    def init(self, data, doi_file, date_file, orcid_file, issn_file, orcid, no_api, id_type):\n",
    "        valid_doi, id_date, id_orcid, id_issn = \\\n",
    "            FileDataHandler._create_csv(doi_file, date_file, orcid_file, issn_file)\n",
    "\n",
    "        if id_type == OCIManager.doi_type:\n",
    "            self.id_manager = DOIManager(valid_doi, use_api_service=not no_api)\n",
    "            crossref_rf = CrossrefResourceFinder(\n",
    "                date=id_date, orcid=id_orcid, issn=id_issn, doi=valid_doi, use_api_service=not no_api )\n",
    "            datacite_rf = DataCiteResourceFinder(\n",
    "                date=id_date, orcid=id_orcid, issn=id_issn, doi=valid_doi, use_api_service=not no_api )\n",
    "            orcid_rf = ORCIDResourceFinder(\n",
    "                date=id_date, orcid=id_orcid, issn=id_issn, doi=valid_doi,\n",
    "                use_api_service=True if orcid is not None and not no_api else False, key=orcid )\n",
    "            self.rf_handler = ResourceFinderHandler( [crossref_rf, datacite_rf, orcid_rf] )\n",
    "\n",
    "\n",
    "        elif id_type == OCIManager.pmid_type:\n",
    "            self.id_manager = PMIDManager(valid_doi, use_api_service=not no_api)\n",
    "            nih_rf = NIHResourceFinder(\n",
    "                date=id_date, orcid=id_orcid, issn=id_issn, pmid=valid_doi, use_api_service=not no_api )\n",
    "            self.rf_handler = ResourceFinderHandler( [nih_rf] )\n",
    "\n",
    "        else:\n",
    "            print(\"the id_type specified is not compliant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.8 storer/update.py <a class=\"anchor\" id=\"en_1.2.8\"></a>\n",
    "Aggiunta di una nuova **funzione per rimuovere triple dal triplestore**.\n",
    "Conseguente aggiunta di un **ulteriore parametro che specifica ogni quante triple effettuare la rimozione dal triplestore**. L'aggiunta del nuovo parametro è giustificata dal fatto che in SPARQL esiste un'operazione per aggiungere con una sola query tutte le triple contenute in un file ma non l'operazione inversa. Di conseguenza, la rimozione richiede un parsing di un file in un grafo, che poi viene iterato. Le triple indicate vengono quindi eliminate singolarmente (o in gruppi) con l'operazione DELETE DATA. \n",
    "*Probabilmente non da aggiungere nel merge, perché pensato per il mapping*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(server, g_url, f_n, date_str, type_file, n):\n",
    "    print(\"REMOVE\")\n",
    "    server = SPARQLWrapper(server)\n",
    "    server.method = 'POST'\n",
    "    file_path = pathlib.Path(abspath(f_n)).as_uri()\n",
    "\n",
    "    #remove all the triples from the specified file\n",
    "    g = Graph()\n",
    "    g.parse(file_path, format=\"nt11\" )\n",
    "\n",
    "    i = 0\n",
    "    triples_group = \"\"\n",
    "\n",
    "    for index, (s, p, o) in enumerate(g):\n",
    "        triple = \"<\" + str( s ) + \">\" + \"<\" + str( p ) + \">\" + \"<\" + str( o ) + \">\" + \".\"\n",
    "        i += 1\n",
    "        if i == int(n):\n",
    "            triples_group = triples_group + triple + \" \"\n",
    "            i = 0\n",
    "            my_query = 'DELETE DATA {GRAPH <' + g_url + '> {' + triples_group + '} }'\n",
    "            server.setQuery(my_query)\n",
    "            server.query()\n",
    "            triples_group = \"\"\n",
    "\n",
    "        else:\n",
    "            triples_group = triples_group + triple + \" \"\n",
    "\n",
    "    if triples_group != \"\":\n",
    "        triples_group = triples_group + triple + \" \"\n",
    "        my_query = 'DELETE DATA {GRAPH <' + g_url + '> {' + triples_group + '} }'\n",
    "        server.setQuery( my_query )\n",
    "        server.query()\n",
    "\n",
    "    with open(\"updatetp_report_%s_%s.txt\" % (type_file, date_str), \"a\",\n",
    "              encoding=\"utf8\") as h:\n",
    "        h.write(\"Removed triples from file '%s'\\n\" % f_n)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    arg_parser = ArgumentParser(\"updatetp.py\", description=\"Update a triplestore with a given \"\n",
    "                                                           \"input .nt/.ttl file of new triples and \"\n",
    "                                                           \"the graph enclosing them. Use .ttl file \"\n",
    "                                                           \"if you need to preserve UTF-8 encoding.\")\n",
    "    arg_parser.add_argument(\"-s\", \"--sparql_endpoint\",\n",
    "                            dest=\"se_url\", required=True,\n",
    "                            help=\"The URL of the SPARQL endpoint.\")\n",
    "    arg_parser.add_argument(\"-i\", \"--input_file\", dest=\"input_file\", required=True,\n",
    "                            help=\"The path to the NT file to upload on the triplestore.\")\n",
    "    arg_parser.add_argument(\"-i_r\", \"--input_file_r\", dest=\"input_file_r\", required=True,\n",
    "                            help=\"The path to the NT file whose triples are to remove from the triplestore.\")\n",
    "    arg_parser.add_argument(\"-g\", \"--graph\", dest=\"graph_name\", required=True,\n",
    "                            help=\"The graph URL to associate to the triples.\")\n",
    "    arg_parser.add_argument(\"-f\", \"--force\", dest=\"force\", default=False, action=\"store_true\",\n",
    "                            help=\"Force the creation of the triples associated to the input graph.\")\n",
    "    arg_parser.add_argument(\"-n\", \"--number\", dest=\"number\", required=True,\n",
    "                            help=\"Number of triples after which the query to remove triples from the triplestore \"\n",
    "                                 \"is performed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.9 cnc.py<a class=\"anchor\" id=\"en_1.2.9\"></a>\n",
    "Aggiunta della scelta \"noci\" per il parametro --pclass, per garantire il corretto recupero dei dati dalla datasource del NIH-OCC.\n",
    "CNC è stato testato con successo anche per NOCI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_parser.add_argument( \"-c\", \"--pclass\", required=True,\n",
    "                         help=\"The name of the class of data source to use to process citatation data.\",\n",
    "                         choices=['csv', 'crossref', 'croci', 'noci'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_cnc.py (estensione per NOCI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateNewCitationsTestPmid(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.idbaseurl = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
    "        self.baseurl = \"https://w3id.org/oc/index/noci/\"#per ora non porta a niente\n",
    "        self.python = \"index%scitation%scitationsource.py\" % (sep, sep)\n",
    "        self.pclass = \"csv\"\n",
    "        self.input = \"index%stest_data%scitations_partial_pmid.csv\" % (sep, sep)\n",
    "        self.pmid_file = \"index%stest_data%scnc_valid_pmid.csv\" % (sep, sep)\n",
    "        self.date_file = \"index%stest_data%scnc_id_date_pmid.csv\" % (sep, sep)\n",
    "        self.orcid_file = \"index%stest_data%scnc_id_orcid_pmid.csv\" % (sep, sep)\n",
    "        self.issn_file = \"index%stest_data%scnc_id_issn_pmid.csv\" % (sep, sep)\n",
    "        self.orcid = None\n",
    "        self.lookup = \"index%stest_data%slookup_full.csv\" % (sep, sep)\n",
    "        self.data = \"index%stest_data%stmp_workflow_pmid\" % (sep, sep)\n",
    "        self.prefix = \"0160\"\n",
    "        self.agent = \"https://w3id.org/oc/index/prov/ra/1\"\n",
    "        self.source = \"https://doi.org/10.35092/yhjc.c.4586573.v16\"\n",
    "        self.service = \"OpenCitations Index: NOCI\"\n",
    "        self.verbose = True\n",
    "        self.no_api = False\n",
    "        self.id_type = OCIManager.pmid_type\n",
    "\n",
    "\n",
    "        self.citation_list = self.__load_citations(\"index%stest_data%scitations_pmid_data.csv\" % (sep, sep),\n",
    "                                                   \"index%stest_data%scitations_pmid_prov.csv\" % (sep, sep))\n",
    "        self.data_path = self.data + sep + \"data\" + sep + \"**\" + sep + \"*.csv\"\n",
    "        self.prov_path = self.data + sep + \"prov\" + sep + \"**\" + sep + \"*.csv\"\n",
    "\n",
    "        if exists(self.data):\n",
    "            rmtree(self.data)\n",
    "\n",
    "    def __load_citations(self, data, prov):\n",
    "        return CitationStorer.load_citations_from_file(data, prov, baseurl=\"https://pubmed.ncbi.nlm.nih.gov/\",\n",
    "            service_name=self.service, id_type=\"pmid\",\n",
    "            id_shape=\"https://pubmed.ncbi.nlm.nih.gov/([[XXX]])\", citation_type=None) #NOTE: discuss id_shape -- il doi può arrivare con caratteri strani,, dvo decodarlo\n",
    "#xxx sostutuisce con l'identificativo di quella specifica citazione\n",
    "\n",
    "    def __citations_csv(self, origin_citation_list, stored_citation_list):\n",
    "        l1 = [cit.get_citation_csv() for cit in origin_citation_list]\n",
    "        l2 = [cit.get_citation_csv() for cit in stored_citation_list]\n",
    "        self.assertEqual(len(l1), len(l2))\n",
    "        self.assertEqual(set(l1), set(l2))\n",
    "\n",
    "    def __test_citations(self):\n",
    "        data_csv = glob(self.data_path, recursive=True)\n",
    "        prov_csv = glob(self.prov_path, recursive=True)\n",
    "        self.assertEqual(len(data_csv), 1)\n",
    "        self.assertEqual(len(prov_csv), 1)\n",
    "        self.__citations_csv(self.citation_list, self.__load_citations(data_csv[0], prov_csv[0]))\n",
    "\n",
    "    def test_execute_workflow(self):\n",
    "        new_citations_added, citations_already_present, error_in_pmids_existence = \\\n",
    "            execute_workflow( self.idbaseurl, self.baseurl, self.pclass, self.input, self.pmid_file,\n",
    "                              self.date_file, self.orcid_file, self.issn_file, self.orcid, self.lookup, self.data,\n",
    "                              self.prefix, self.agent, self.source, self.service, self.verbose, self.no_api, 1, self.id_type)\n",
    "\n",
    "        self.assertEqual(new_citations_added, 6)\n",
    "        self.assertEqual(citations_already_present, 0)\n",
    "        self.assertEqual(error_in_pmids_existence, 0)\n",
    "        self.__test_citations()\n",
    "\n",
    "        new_citations_added, citations_already_present, error_in_pmids_existence = \\\n",
    "            execute_workflow( self.idbaseurl, self.baseurl, self.pclass, self.input, self.pmid_file,\n",
    "                              self.date_file, self.orcid_file, self.issn_file, self.orcid, self.lookup, self.data,\n",
    "                              self.prefix, self.agent, self.source, self.service, self.verbose, self.no_api, 1, self.id_type)\n",
    "        self.assertEqual(new_citations_added, 0)\n",
    "        self.assertEqual(citations_already_present, 6)\n",
    "        self.assertEqual(error_in_pmids_existence, 0)\n",
    "        self.__test_citations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "Ricontrollare il processo di generazione OCI per PMID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAMOSE - NOCI configuration file<a class=\"anchor\" id=\"en_2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAMOSE - indexapi.py <a class=\"anchor\" id=\"en_2.2\"></a>\n",
    "Sviluppo di funzioni aggiuntive per l'operazione **metadata**. Le funzioni sviluppate permettono di recuperare metadati specifici per i PMID dal servizio API offerto dal NIH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __nih_parser(pmid):\n",
    "    api = \"https://pubmed.ncbi.nlm.nih.gov/%s\"\n",
    "    pmid_sep = str(pmid) + \"%s\"\n",
    "    display_opt = \"/?format=pubmed\"\n",
    "\n",
    "    try:\n",
    "        r = get(api % pmid_sep % display_opt,\n",
    "                headers={\"User-Agent\": \"NOCI REST API (via OpenCitations - \"\n",
    "                                       \"http://opencitations.net; mailto:contact@opencitations.net)\"}, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            r.encoding = \"utf-8\"\n",
    "            soup = BeautifulSoup(r.text, features=\"lxml\")\n",
    "            body = str(soup.find(id=\"article-details\"))\n",
    "\n",
    "            authors = _get_author_nih(body)\n",
    "\n",
    "\n",
    "            year = \"\"\n",
    "            nih_date = _get_date_nih(body)\n",
    "            if nih_date is not None:\n",
    "                year = __normalise(nih_date)\n",
    "\n",
    "            title = \"\"\n",
    "            nih_title = _get_title_nih(body)\n",
    "            if nih_title is not None:\n",
    "                title = __create_title(nih_title)\n",
    "\n",
    "            source_title = \"\"\n",
    "            nih_cont_title = _get_cont_title_nih(body)\n",
    "            if nih_cont_title is not None:\n",
    "                source_title = __create_title(nih_cont_title)\n",
    "\n",
    "            volume = \"\"\n",
    "            issue = \"\"\n",
    "            page = \"\"\n",
    "            source_id = \"\"\n",
    "\n",
    "            print ([\"; \".join(authors), year, title, source_title, volume, issue, page, source_id])\n",
    "            return [\"; \".join(authors), year, title, source_title, volume, issue, page, source_id]\n",
    "\n",
    "    except Exception as e:\n",
    "        pass  # do nothing\n",
    "\n",
    "def _get_author_nih(txt_obj):\n",
    "    result = []\n",
    "    authors = re.findall(\"FAU\\s+-\\s+((([A-Z])\\w*('|-|\\.)?)(\\s*([A-Z])\\w*('|-|\\.)?)*,\\s*(([A-Z])([^\\S\\r\\n][A-Z])*))\", txt_obj)\n",
    "    for i in authors:\n",
    "        author = re.search(\"(([A-Z])\\w*('|-|\\.)?)(\\s*([A-Z])\\w*('|-|\\.)?)*,\\s*(([A-Z])([^\\S\\r\\n][A-Z])*)\", str(i)).group(0)\n",
    "        result.append(__normalise(author))\n",
    "    return result\n",
    "\n",
    "def _get_title_nih(txt_obj):\n",
    "    title = re.search(\"TI\\s+-\\s+([^\\n]+)\", txt_obj).group(1)\n",
    "    re_search = re.search(\"([^\\n]+)\", title)\n",
    "    if re_search is not None:\n",
    "        result = re_search.group(0)\n",
    "    return result\n",
    "\n",
    "def _get_cont_title_nih(txt_obj):\n",
    "    cont_title = re.search(\"JT\\s+-\\s+([^\\n]+)\", txt_obj).group(1)\n",
    "    re_search = re.search(\"([^\\n]+)\", cont_title)\n",
    "    if re_search is not None:\n",
    "        result = re_search.group(0)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _get_date_nih(txt_obj):\n",
    "    date = re.search(\"DP\\s+-\\s+(\\d{4}(\\s?(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec))?(\\s?((3[0-1])|([1-2][0-9])|([0]?[1-9])))?)\", txt_obj).group(1)\n",
    "    re_search = re.search(\"(\\d{4})\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+((3[0-1])|([1-2][0-9])|([0]?[1-9]))\", date)\n",
    "    if re_search is not None:\n",
    "        result = re_search.group(0)\n",
    "        datetime_object = datetime.strptime(result, '%Y %b %d')\n",
    "        return datetime.strftime(datetime_object, '%Y-%m-%d')\n",
    "    else:\n",
    "        re_search = re.search(\"(\\d{4})\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\", date)\n",
    "        if re_search is not None:\n",
    "            result = re_search.group(0)\n",
    "            datetime_object = datetime.strptime(result, '%Y %b')\n",
    "            return datetime.strftime(datetime_object, '%Y-%m')\n",
    "        else:\n",
    "            re_search = re.search(\"(\\d{4})\", date)\n",
    "            if re_search is not None:\n",
    "                result = re.search(\"(\\d{4})\", date).group(0)\n",
    "                datetime_object = datetime.strptime(result, '%Y')\n",
    "                return datetime.strftime(datetime_object, '%Y')\n",
    "            else:\n",
    "                return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09/03 - 15/03 (_________) <a class=\"anchor\" id=\"entry_3\"></a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15/03 - 22/03 (Prometheus file format study and data extraction) <a class=\"anchor\" id=\"entry_4\"></a>\n",
    "\n",
    "Lorem Ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prometheus File Format -  metrics / OpenMetrics types\n",
    "\n",
    "<ul>\n",
    "    <li> <b>Counter</b>: cumulative metric that only increases over time, like the number of requests to an endpoint.</li>\n",
    "    <li><b>Gauge</b>: instantaneous measurements of a value. They can be arbitrary values which will be recorded. Gauges represent a random value that can increase and decrease randomly such as the load of your system. </li>\n",
    "    <li><b>Histogram</b>: A histogram samples observations (usually things like request durations or response sizes) and counts them in configurable buckets. It also provides a sum of all observed values. A histogram with a base metric name of exposes multiple time series during a scrape</li>\n",
    "    <li><b>Summary</b>: Similar to a histogram, a summary samples observations (usually things like request durations and response sizes). While it also provides a total count of observations and a sum of all observed values, it calculates configurable quantiles over a sliding time window. A summary with a base metric name of also exposes multiple time series during a scrape</li>\n",
    "</ul>\n",
    "\n",
    "#### Fonti:\n",
    "<ol>\n",
    "    <li><a href=\"https://sysdig.com/blog/prometheus-metrics/\">sysdig.com</a></li>\n",
    "    <li><a href=\"https://www.udemy.com/share/103iRG3@78CD2xvy-1vMXY-ZXPgzP9dG4nSXmHE9zDaRoUur8Hjc0sCTnKU2KEwoWDaqUaQJXg==/\">udemy.com</a></li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prometheus File Format - data extraction\n",
    "Codice per estrarre dati dal prometheus file format restituito dall'API per i dati di log in OC.\n",
    "Il codice permette di accedere al nome della metrica, alle labels associate e ai suoi valori. \n",
    "#### Fonti:\n",
    "<ol>\n",
    "    <li><a href=\"https://python.hotexamples.com/it/examples/prometheus_client.parser/-/text_string_to_metric_families/python-text_string_to_metric_families-function-examples.html\">python.hotexamples.com</a></li>\n",
    "    <li> stackoverflow.com for<a href=\"https://stackoverflow.com/questions/60050507/reading-prometheus-metric-using-python\"> reading prometheus metric</a> and for <a href=\"https://stackoverflow.com/questions/988228/convert-a-string-representation-of-a-dictionary-to-a-dictionary\"> reading a string dictionary as a dictionary</a></li>\n",
    "    <li><a href=\"https://www.robustperception.io/productive-prometheus-python-parsing\">/www.robustperception.io</a></li>\n",
    "    <li><a href=\"https://www.robustperception.io/productive-prometheus-python-parsing\">/www.robustperception.io</a></li>\n",
    "    \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from urllib.parse import quote\n",
    "from json import loads\n",
    "import json\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "import datetime\n",
    "import time\n",
    "from prometheus_client.parser import text_string_to_metric_families\n",
    "import requests\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: opencitations_http_requests_total\n",
      "endpoint: /index/api/v1/\n",
      "metric: opencitations_http_requests_total\n",
      "endpoint: /ccc/\n",
      "metric: opencitations_http_requests_total\n",
      "endpoint: /ccc/sparql\n",
      "metric: opencitations_http_requests_total\n",
      "endpoint: /index/coci/ci/\n",
      "metric: opencitations_http_requests_total\n",
      "endpoint: /sparql\n",
      "metric: opencitations_http_requests_total\n",
      "endpoint: /index/coci/api/v1/\n",
      "metric: opencitations_http_requests_total\n",
      "endpoint: /api/v1/\n",
      "metric: opencitations_http_requests_total\n",
      "endpoint: /intrepid\n",
      "metric: opencitations_http_requests_total\n",
      "endpoint: /index/croci/ci/\n",
      "metric: opencitations_http_requests_total\n",
      "endpoint: /index/croci/api/v1/\n",
      "metric: opencitations_http_requests_total\n",
      "endpoint: /oci\n",
      "metric: opencitations_http_requests_total\n",
      "endpoint: /ccc/api/v1/\n",
      "metric: opencitations_http_requests_total\n",
      "endpoint: /corpus/\n",
      "metric: opencitations_http_requests_total\n",
      "endpoint: /index/sparql\n"
     ]
    }
   ],
   "source": [
    "url = \"http://opencitations.net/statistics/\"\n",
    "gen = \"01\"\n",
    "feb = \"02\"\n",
    "mar = \"03\"\n",
    "apr = \"04\"\n",
    "mag = \"05\"\n",
    "giu = \"06\"\n",
    "lug = \"07\"\n",
    "ago = \"08\"\n",
    "sep = \"09\"\n",
    "ott = \"10\"\n",
    "nov = \"11\"\n",
    "dic = \"12\"\n",
    "y_21 = \"2021\"\n",
    "y_22 = \"2022\"\n",
    "y_20 = \"2020\"\n",
    "\n",
    "def call_api(year, month, api):\n",
    "    PROMETHEUS = api + year + \"-\" + month\n",
    "    metrics = requests.get(PROMETHEUS).content.decode('utf-8')\n",
    "    #print(metrics)\n",
    "    for family in text_string_to_metric_families(metrics):\n",
    "        #print(family)\n",
    "        for sample in family.samples:\n",
    "            #print(\"Name: {0} Labels: {1} Value: {2}\".format(*sample))\n",
    "            if \"opencitations_http_requests_total\" in \"Name:{0}\".format(*sample):\n",
    "                print(\"metric: opencitations_http_requests_total\")\n",
    "                res = \"{1}\".format(*sample)\n",
    "                labels_dict = ast.literal_eval(res)\n",
    "                for k,v in labels_dict.items():\n",
    "                    if k == \"endpoint\":\n",
    "                        print(\"endpoint:\", v)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "call_api(y_22, gen, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spiegazione dei parametri del Prometheus file format in OC\n",
    "<ol>\n",
    "    <li><b></b></li>\n",
    "    <li><b></b></li>\n",
    "</ol>\n",
    "\n",
    "Output file format\n",
    "The output file contains the following fields:\n",
    "\n",
    "<ol>\n",
    "    <li><b>opencitations_http_requests</b>: counter of http requests. It is represented by a Counter in prometheus where each label indicates a counter for a specific endpoint, e.g. opencitations_http_requests_total{endpoint=\"/index/croci/ci/\"} is the the counter for the endpoint /index/croci/ci/.</li>\n",
    "    <li><b>opencitations_http_requests_created</b>: contains the timestamp indicating the creation date of each counter opencitations_http_requests{*}.</li>\n",
    "        <li><b>opencitations_agg_counter_total</b>:  aggregate counter of http requests. It is represented by a Counter in prometheus where each label indicates a counter for a specific aggregation, endpoints are grouped as follows: \n",
    "            <ol>\n",
    "                <li><b>opencitations_agg_counter_total{category=\"sparql_requests\"}</b></li>\n",
    "                    <ol>\n",
    "                        <li>/sparql</li>\n",
    "                        <li>/index/sparql</li>\n",
    "                        <li>/ccc/sparql</li>\n",
    "                    </ol>\n",
    "                <li><b>opencitations_agg_counter_total{category=\"additional_services_requests\"}</b></li>\n",
    "                <ol>\n",
    "                    <li>/oci</li>\n",
    "                    <li>/intrepid</li>\n",
    "                </ol>\n",
    "            <li><b>opencitations_agg_counter_total{category=\"dataset_requests\"}</b></li>\n",
    "                <ol>\n",
    "                    <li>/corpus/</li>\n",
    "                    <li>/index/coci/ci/</li>\n",
    "                    <li>/index/croci/ci/</li>\n",
    "                    <li>/ccc/</li>\n",
    "                </ol>\n",
    "                <li><b>opencitations_agg_counter_total{category=\"oc_api_requests\"}</b></li>\n",
    "                <ol>\n",
    "                    <li>/index/api/v1/</li>\n",
    "                    <li>/index/coci/api/v1/</li>\n",
    "                    <li>/index/croci/api/v1/</li>\n",
    "                    <li>/ccc/api/v1/</li>\n",
    "                    <li>/api/v1/</li>\n",
    "                </ol>\n",
    "            </ol>\n",
    "    <li><b>agg_counter_created:</b>contains the timestamp indicating the creation date of each aggregated counter opencitations_agg_counter_total{*}. </li>\n",
    "        <li><b>opencitations_harvested_data_sources</b></li>\n",
    "        <li><b>opencitations_indexed_records</b></li>\n",
    "    </ol>\n",
    "\n",
    "\n",
    "#### Fonte\n",
    "<a href=\"\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domande\n",
    "<ol>\n",
    "    <li>Seguendo l'approccio adottato finora, pensavo di estrarre i dati dal file format e utilizzarli in delle visualizzazioni in d3.js. Non ho usato prometheus client library nell'approccio esposto sopra (ho fatto qualche tentativo ma ho capito meglio la parte per intervenire sul formato rispetto a quella per estrarre i dati). </li>\n",
    "       <li> Per le visualizzazioni sarebbe meglio utilizzare qualcosa di più specifico (Sysdig ?) o è sufficiente estrarre i dati e seguire un approccio generale (che avrei utilizzato con qualsiasi altro tipo di dati estratti da un altro tipo di file format?) </li>\n",
    "    <li>Dovrei fare un uso specifico di statistics/script? </li>\n",
    "    <li>opencitations_harvested_data_sources\n",
    "opencitations_indexed_records non hanno spiegazione</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ??/?? - ??/?? (????) <a class=\"anchor\" id=\"entry_5\"></a>\n",
    "????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
